---
title: "Efficiently Engineering Bigger Data with Arrow"
author: "Nic Crane"
format: revealjs
execute:
  echo: true
engine: knitr
---

# Overview

-   Demo of working with larger-than-memory data in R with Arrow
-   What is Arrow?
-   Using the dplyr API with arrow
-   Using Parquet format for better performance
-   Dataset partitioning

## What is larger-than-memory data?

![Source: July 2023 data from https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam](images/larger-than-memory.png)

## Goals

Avoiding these! But...don't worry!

![](images/segfault.png)

## NYC Taxi Dataset

![](images/nyc-taxi-homepage.png){.absolute left="200" width="600"}

::: {style="font-size: 60%; margin-top: 550px; margin-left: 200px;"}
<https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page>
:::

## NYC Taxi Data

- Subset of the NYC Taxi data set on S3 (\~40GBs on disk)

```{r}
#| label: get-big-data
#| eval: false
library(arrow)
library(dplyr)

open_dataset("s3://voltrondata-labs-datasets/nyc-taxi") |>
  filter(year %in% 2012:2021) |>
  write_dataset("data/nyc-taxi", partitioning = c("year", "month"))
```

## Demo 1

{{< video helloarrow.mp4 >}}

## NYC Taxi Dataset

```{r}
#| label: setup
#| echo: false
#| warning: false
library(arrow)
library(dplyr)
library(tictoc)
```

```{r}
#| label: first-taxi-data
library(arrow)
nyc_taxi <- open_dataset("~/data/nyc-taxi/")

nyc_taxi |> 
  nrow()
```

<br>

1.15 billion rows!

## NYC Taxi Dataset: A {dplyr} pipeline

```{r}
#| label: first-collect
#| eval: false
library(dplyr)
library(tictoc)

tic()
nyc_taxi |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```

> 6.077 sec elapsed

# What is Apache Arrow?

> A multi-language toolbox for accelerated data interchange and in-memory processing

::: {style="font-size: 70%;"}
<https://arrow.apache.org/overview/>
:::

## Apache Arrow Specification

In-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.

<br>

![](images/arrow-rectangle.png){.absolute left="200"}

## A Multi-Language Toolbox

![](images/arrow-libraries-structure.png)

## Accelerated Data Interchange

![](images/data-interchange-with-arrow.png)

## Accelerated In-Memory Processing

Arrow's Columnar Format is Fast

![](images/columnar-fast.png){.absolute top="120" left="200" height="600"}

::: notes
The contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors.
:::

## arrow ðŸ“¦

<br>

![](images/arrow-r-pkg.png){.absolute top="0" left="300" width="700" height="900"}


# dplyr API in arrow

![(Danielle Navarro, 2022 - https://arrow-user2022.netlify.app). CC BY-SA 4.0](images/dplyr-backend.png)

## arrow supports complex expressions

Bindings to functions from lubridate, stringr, and others

```{r}
#| label: bindings
nyc_taxi |>
  mutate(
    pickup_morning = lubridate::am(pickup_datetime), 
    across(ends_with("amount"), list(pounds = ~.x * 0.79)) 
  ) |>
  filter(
    stringr::str_ends(vendor_name, "S"),
    year == 2019,
    month == 9
  ) |>
  select(
    pickup_morning,
    contains("amount"),
    everything()
  ) |>
  head() |>
  collect()
```

## head to the docs!

```{r}
#| label: get-help
#| eval: false

?`arrow-dplyr`
```

{{< video helppage.mp4 >}}

## DuckDB

![](images/dplyr-arrow-duckdb.png)

## Arrow Datasets

-   Similar to database connections
-   Can consist of multiple files
-   Lazy evaluation

## Arrow Dataset Objects

![](images/nyc_taxi_dataset.png)

# Parquet

![](images/parquet.png)

-   usually smaller than equivalent CSV file
-   rich type system & stores the metadata along with the data
-   "column-oriented" == better performance over CSV's row-by-row
-   "row-chunked" == work on different parts of the file at the same time or skip some chunks all together

## Let's compare!

```{r}
#| label: write_dataset
#| eval: false
nyc_taxi |>
  write_dataset(
    "~/data/nyc-taxi-csv",
    format = "csv", 
    partitioning = c("year", "month")
  )
```

## Comparing size on disk

```{r}
#| label: total_file_size
total_file_size <- function(path){
  all_files_in_dir <- fs::dir_ls(path, recurse = TRUE)
  sum(fs::file_size(all_files_in_dir))
}
```

## How much space do the CSV and Parquet versions of the same files take up on disk?

```{r}
#| label: parquet_size

# Parquet files
total_file_size("~/data/nyc-taxi/")
```

```{r}
#| label: csv_size

# CSV files
total_file_size("~/data/nyc-taxi-csv/")
```

## Comparing analysis speed - Parquet

```{r}
#| label: parquet_time
#| eval: false
# Parquet
tic()
open_dataset("~/data/nyc-taxi") |>
  filter(year %in% 2014:2017) |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```

> 2.54 sec elapsed

## Comparing analysis speed - CSV

```{r}
#| label: csv_time
#| eval: false
#CSV
tic()
open_dataset("~/data/nyc-taxi-csv", format = "csv") |>
  filter(year %in% 2014:2017) |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 49.788 sec elapsed

## Parquet file metadata

![Source: https://parquet.apache.org/docs/file-format/metadata/](images/parquet_metadata.gif)

# Partitioning

- Data is saved in directories representing columns in the data

![](images/partitions.png)


## What variables should we use for partitioning?

What happens if we just partition by year and not month?
```{r}
#| eval: false
nyc_taxi |>
  write_dataset("~/data/nyc-taxi-year", partitioning = "year")
```

Let's compare!

```{r}
#| label: open-both

year_part <- open_dataset("~/data/nyc-taxi-year")
year_month_part <- open_dataset("~/data/nyc-taxi")
```

## Partitioned by year

```{r}
#| eval: false
tic()
 year_part |>
 filter(year %in% 2014:2017) |>
  group_by(year, month) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 8.338 sec elapsed

## Partitioned by both month and year

```{r}
#| eval: false
tic()
 year_month_part |>
 filter(year %in% 2014:2017) |>
  group_by(year, month) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 5.621 sec elapsed

## Art & Science of Partitioning

-   Number of partitions also important (Arrow reads the metadata of each file)
-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (ðŸ¤¯)
-   partition on variables used in `filter()`

# Summary

1.  Arrow lets you work with larger-than-memory data directly from R using the dplyr API
2.  Store files in Parquet format for better performance
3.  Partition data based on your analysis workflows

# Resources

## Docs

[![https://arrow.apache.org/docs/r/](images/docs.png)](https://arrow.apache.org/docs/r/)

## Cookbook

[![https://arrow.apache.org/cookbook/r/](images/cookbook.png)](https://arrow.apache.org/cookbook/r/)

## Cheatsheet

[![https://github.com/apache/arrow/blob/main/r/cheatsheet/arrow-cheatsheet.pdf](https://arrow.apache.org/img/20220427-arrow-r-cheatsheet-thumbnail.png)](https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf)

## posit::conf 2023 tutorial

[![https://posit-conf-2023.github.io/arrow/](images/posit-workshop.png)](https://posit-conf-2023.github.io/arrow/)


## Awesome Arrow

[![https://github.com/thisisnic/awesome-arrow-r](images/awesomearrow.png)](https://github.com/thisisnic/awesome-arrow-r)

# Get Involved!

## Open an issue

[![https://github.com/apache/arrow/issues/](images/issues.png)](https://github.com/apache/arrow/issues/)

## Make a PR!

-   docs
-   cookbook
-   code

# Efficiently Engineering Bigger Data with Arrow