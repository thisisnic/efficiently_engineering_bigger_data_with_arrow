---
title: "Efficiently Engineering Bigger Data with Arrow"
author: "Nic Crane"
format: revealjs
execute:
  echo: true
engine: knitr
---

# Overview

-   Working with larger-than-memory data in R with Arrow
-   Using the dplyr API with arrow
-   Using Parquet format for better performance
-   Dataset partitioning

## What is larger-than-memory data?

![Source: July 2023 data from https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam](images/larger-than-memory.png)

## NYC Taxi Dataset

![](images/nyc-taxi-homepage.png){.absolute left="200" width="600"}

::: {style="font-size: 60%; margin-top: 550px; margin-left: 200px;"}
<https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page>
:::

## NYC Taxi Data

- Subset of the NYC Taxi data set on S3 (\~40GBs on disk)

```{r}
#| label: get-big-data
#| eval: false
library(arrow)
library(dplyr)

open_dataset("s3://voltrondata-labs-datasets/nyc-taxi") |>
  filter(year %in% 2012:2021) |>
  write_dataset("data/nyc-taxi", partitioning = c("year", "month"))
```

## Demo 1

{{< video helloarrow.mp4 >}}

## NYC Taxi Dataset

```{r}
#| label: setup
#| echo: false
#| warning: false
library(arrow)
library(dplyr)
library(tictoc)
```

```{r}
#| label: first-taxi-data
library(arrow)
nyc_taxi <- open_dataset("~/data/nyc-taxi/")

nyc_taxi |> 
  nrow()
```

<br>

1.15 billion rows!

## NYC Taxi Dataset: A {dplyr} pipeline

```{r}
#| label: first-collect
#| eval: false
library(dplyr)
library(tictoc)

tic()
nyc_taxi |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```

> 6.077 sec elapsed

## What is Apache Arrow?


> A multi-language toolbox for accelerated data interchange and in-memory processing

::: {style="font-size: 70%;"}
<https://arrow.apache.org/overview/>
:::

## Apache Arrow Specification

In-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.

<br>

![](images/arrow-rectangle.png){.absolute left="200"}

## A Multi-Language Toolbox

![](images/arrow-libraries-structure.png)

## Accelerated Data Interchange

![](images/data-interchange-with-arrow.png)

## Accelerated In-Memory Processing

Arrow's Columnar Format is Fast

![](images/columnar-fast.png){.absolute top="120" left="200" height="600"}

::: notes
The contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors.
:::

## arrow üì¶

<br>

![](images/arrow-r-pkg.png){.absolute top="0" left="300" width="700" height="900"}


## dplyr API in arrow

![(Danielle Navarro, 2022 - https://arrow-user2022.netlify.app). CC BY-SA 4.0](images/dplyr-backend.png)

## Arrow Datasets

-   Similar to database connections
-   Can consist of multiple files
-   Lazy evaluation

![Image source: Navarro, Danielle. 2022. ‚ÄúUnpacking Arrow Datasets.‚Äù November 30, 2022. https://blog.djnavarro.net/posts/2022-11-30_unpacking-arrow-datasets.](images/datasets.png)

## Arrow Dataset Objects

```{r}
nyc_taxi
```

## Constructing a query

```{r}
#| label: shared-rides
library(dplyr)
shared_rides <- nyc_taxi |>
  group_by(year) |>
  summarize(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) 

class(shared_rides)
```
## arrow dplyr queries

- query has been constructed but not evaluated
- nothing has been pulled into memory
- call `collect()` to pull data into R session

## Tips for working with large datasets

Task: Convert fares to GBP (¬£)

```{r}
#| label: fares-gbp

fares_pounds <- nyc_taxi |>
  filter(year %in% 2019:2021) |>
  mutate(
    fare_amount_pounds = fare_amount * 0.79
  ) |>
  select(fare_amount, fare_amount_pounds)
```

## Use `nrow()` to see how many rows
```{r}
#| label: fares-gbp-nrow

fares_pounds |>
  nrow()
```

139 million rows of data

## Use `head()` and `dplyr::collect()` to preview results

```{r}
#| label: fares-gbp-head
fares_pounds |>
  head() |>
  collect()
```

## dplyr bindings

![](images/dplyr_implemented.png)

## What if a binding doesn't exist?

First three trips in the dataset in 2021 where distance \> 100 miles

```{r}
#| label: no-slice
#| error: true

long_rides_2021 <- nyc_taxi |>
  filter(year == 2021 & trip_distance > 100) |>
  select(pickup_datetime, year, trip_distance)

long_rides_2021 |>
  slice(1:3)
```
## head to the docs!

```{r}
#| label: get-help
#| eval: false

?arrow-dplyr
```


## A different function

```{r}
#| label: slice-max

long_rides_2021 |>
  slice_head(n = 3) |>
  collect()
```

## Or call `collect()` first

```{r}
#| label: collect-first

long_rides_2021 |>
  collect() |>
  slice(1:3)
```

# Window functions

## What are window functions?

-   calculations within groups

## Grouped summaries

```{r}
#| label: grouped-summaries

fare_by_year <- nyc_taxi |>
  filter(year %in% 2021:2022) |>
  select(year, fare_amount)

fare_by_year |>
  group_by(year) |>
  summarise(mean_fare = mean(fare_amount)) |> 
  collect()
```

## Window functions

```{r}
#| label: window-fail
#| error: true

fare_by_year |>
  group_by(year) |>
  mutate(mean_fare = mean(fare_amount)) |> 
  collect()
```

## DuckDB

![](images/dplyr-arrow-duckdb.png)

## Window functions - via duckdb

<!-- -->

```{r}
#| label: window-duckdb
library(duckdb)

fare_by_year |>
  group_by(year) |>
  to_duckdb() |>
  mutate(mean_fare = mean(fare_amount)) |> 
  to_arrow() |>
  arrange(desc(fare_amount)) |>
  collect()
```

# Parquet

## Parquet

![](images/parquet.png)

-   usually smaller than equivalent CSV file
-   rich type system & stores the metadata along with the data
-   "column-oriented" == better performance over CSV's row-by-row
-   "row-chunked" == work on different parts of the file at the same time or skip some chunks all together

::: notes
-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory
-   CSV has no info about data types, inferred by each parser
:::

## Let's compare!

```{r}
#| label: write_dataset
#| eval: false
nyc_taxi |>
  write_dataset(
    "~/data/nyc-taxi-csv",
    format = "csv", 
    partitioning = c("year", "month")
  )
```

## Comparing size on disk

```{r}
#| label: total_file_size
total_file_size <- function(path){
  all_files_in_dir <- fs::dir_ls(path, recurse = TRUE)
  sum(fs::file_size(all_files_in_dir))
}
```

## How much space do the CSV and Parquet versions of the same files take up on disk?

```{r}
#| label: parquet_size

# Parquet files
total_file_size("~/data/nyc-taxi/")
```

```{r}
#| label: csv_size

# CSV files
total_file_size("~/data/nyc-taxi-csv/")
```

## Comparing analysis speed - Parquet

```{r}
#| label: parquet_time
#| eval: false
# Parquet
tic()
open_dataset("~/data/nyc-taxi") |>
  filter(year %in% 2014:2017) |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```

> 2.54 sec elapsed

## Comparing analysis speed - CSV

```{r}
#| label: csv_time
#| eval: false
#CSV
tic()
open_dataset("~/data/nyc-taxi-csv", format = "csv") |>
  filter(year %in% 2014:2017) |>
  group_by(year) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 49.788 sec elapsed

## Parquet file metadata

![Source: https://parquet.apache.org/docs/file-format/metadata/](images/parquet_metadata.gif)

# Partitioning

- Data is saved in directories representing columns in the data

![](images/partitions.png)


## What variables should we use for partitioning?

What happens if we just partition by year and not month?
```{r}
#| eval: false
nyc_taxi |>
  write_dataset("~/data/nyc-taxi-year", partitioning = "year")
```

Let's compare!

```{r}
#| label: open-both

year_part <- open_dataset("~/data/nyc-taxi-year")
year_month_part <- open_dataset("~/data/nyc-taxi")
```

## Partitioned by year

```{r}
#| eval: false
tic()
 year_part |>
 filter(year %in% 2014:2017) |>
  group_by(year, month) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 8.338 sec elapsed

## Partitioned by both month and year

```{r}
#| eval: false
tic()
 year_month_part |>
 filter(year %in% 2014:2017) |>
  group_by(year, month) |>
  summarise(
    all_trips = n(),
    shared_trips = sum(passenger_count > 1, na.rm = TRUE)
  ) |>
  mutate(pct_shared = shared_trips / all_trips * 100) |>
  collect()
toc()
```
> 5.621 sec elapsed

## Art & Science of Partitioning

-   Number of partitions also important (Arrow reads the metadata of each file)
-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (ü§Ø)
-   partition on variables used in `filter()`

# Summary

1.  Arrow lets you work with larger-than-memory data directly from R using the dplyr API
2.  Store files in Parquet format for better performance
3.  Partition data based on your analysis workflows

```{=html}
<!-- Need to add in all of Danielle's acknowledgements



[![Image from "Larger-Than-Memory Data Workflows with Apache Arrow" by Danielle Navarro is licensed under CC BY-ND 4.0](https://github.com/djnavarro/arrow-user2022/blob/main/img/arrow-libraries-structure.png?raw=true)](https://arrow-user2022.netlify.app/)

## The arrow R package

[![Image from "Larger-Than-Memory Data Workflows with Apache Arrow" by Danielle Navarro is licensed under CC BY-ND 4.0](https://github.com/djnavarro/arrow-user2022/blob/main/img/dplyr-backend.png?raw=true)](https://arrow-user2022.netlify.app/)

-->
```

# Resources

## Docs

[![https://arrow.apache.org/docs/r/](images/docs.png)](https://arrow.apache.org/docs/r/)

## Cookbook

[![https://arrow.apache.org/cookbook/r/](images/cookbook.png)](https://arrow.apache.org/cookbook/r/)

## Cheatsheet

[![https://github.com/apache/arrow/blob/main/r/cheatsheet/arrow-cheatsheet.pdf](https://arrow.apache.org/img/20220427-arrow-r-cheatsheet-thumbnail.png)](https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf)

```{=html}
<!-- 
The Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package‚Äôs main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full Arrow for R package documentation and articles and the Arrow Cookbook, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the Apache Arrow visual identity guidance.
-->
```
## UseR! 2022 Tutorial

[![https://arrow-user2022.netlify.app/](images/usertutorial.png)](https://arrow-user2022.netlify.app/)

<!-- Also mention mini taxi dataset -->

## Awesome Arrow

[![https://github.com/thisisnic/awesome-arrow-r](images/awesomearrow.png)](https://github.com/thisisnic/awesome-arrow-r)

# Get Involved!

## Open an issue

[![https://github.com/apache/arrow/issues/](images/issues.png)](https://github.com/apache/arrow/issues/)

## Make a PR!

-   docs
-   cookbook
-   code

# Efficiently Engineering Bigger Data with Arrow