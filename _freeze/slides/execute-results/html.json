{
  "hash": "0c306b3d3553666f1107ae876d2c8fe2",
  "result": {
    "markdown": "---\ntitle: \"Efficiently Engineering Bigger Data with Arrow\"\nauthor: \"Nic Crane\"\nformat: revealjs\nexecute:\n  echo: true\nengine: knitr\n---\n\n\n# Overview\n\n-   Working with larger-than-memory data in R with Arrow\n-   Using the dplyr API with arrow\n-   Using Parquet format for better performance\n-   Dataset partitioning\n\n## What is larger-than-memory data?\n\n![Source: July 2023 data from https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam](images/larger-than-memory.png)\n\n## NYC Taxi Dataset\n\n![](images/nyc-taxi-homepage.png){.absolute left=\"200\" width=\"600\"}\n\n::: {style=\"font-size: 60%; margin-top: 550px; margin-left: 200px;\"}\n<https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page>\n:::\n\n## NYC Taxi Data\n\n- Subset of the NYC Taxi data set on S3 (\\~40GBs on disk)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(dplyr)\n\nopen_dataset(\"s3://voltrondata-labs-datasets/nyc-taxi\") |>\n  filter(year %in% 2012:2021) |>\n  write_dataset(\"data/nyc-taxi\", partitioning = c(\"year\", \"month\"))\n```\n:::\n\n\n## Demo 1\n\n\n{{< video helloarrow.mp4 >}}\n\n\n\n## NYC Taxi Dataset\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nnyc_taxi <- open_dataset(\"~/data/nyc-taxi/\")\n\nnyc_taxi |> \n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1155795912\n```\n:::\n:::\n\n\n<br>\n\n1.15 billion rows!\n\n## NYC Taxi Dataset: A {dplyr} pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tictoc)\n\ntic()\nnyc_taxi |>\n  group_by(year) |>\n  summarise(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n:::\n\n\n> 6.077 sec elapsed\n\n## What is Apache Arrow?\n\n\n> A multi-language toolbox for accelerated data interchange and in-memory processing\n\n::: {style=\"font-size: 70%;\"}\n<https://arrow.apache.org/overview/>\n:::\n\n## Apache Arrow Specification\n\nIn-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.\n\n<br>\n\n![](images/arrow-rectangle.png){.absolute left=\"200\"}\n\n## A Multi-Language Toolbox\n\n![](images/arrow-libraries-structure.png)\n\n## Accelerated Data Interchange\n\n![](images/data-interchange-with-arrow.png)\n\n## Accelerated In-Memory Processing\n\nArrow's Columnar Format is Fast\n\n![](images/columnar-fast.png){.absolute top=\"120\" left=\"200\" height=\"600\"}\n\n::: notes\nThe contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors.\n:::\n\n## arrow üì¶\n\n<br>\n\n![](images/arrow-r-pkg.png){.absolute top=\"0\" left=\"300\" width=\"700\" height=\"900\"}\n\n\n## dplyr API in arrow\n\n![(Danielle Navarro, 2022 - https://arrow-user2022.netlify.app). CC BY-SA 4.0](images/dplyr-backend.png)\n\n## Arrow Datasets\n\n-   Similar to database connections\n-   Can consist of multiple files\n-   Lazy evaluation\n\n![Image source: Navarro, Danielle. 2022. ‚ÄúUnpacking Arrow Datasets.‚Äù November 30, 2022. https://blog.djnavarro.net/posts/2022-11-30_unpacking-arrow-datasets.](images/datasets.png)\n\n## Arrow Dataset Objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc_taxi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 122 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n```\n:::\n:::\n\n\n## Constructing a query\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nshared_rides <- nyc_taxi |>\n  group_by(year) |>\n  summarize(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) \n\nclass(shared_rides)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"arrow_dplyr_query\"\n```\n:::\n:::\n\n## arrow dplyr queries\n\n- query has been constructed but not evaluated\n- nothing has been pulled into memory\n- call `collect()` to pull data into R session\n\n## Tips for working with large datasets\n\nTask: Convert fares to GBP (¬£)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfares_pounds <- nyc_taxi |>\n  filter(year %in% 2019:2021) |>\n  mutate(\n    fare_amount_pounds = fare_amount * 0.79\n  ) |>\n  select(fare_amount, fare_amount_pounds)\n```\n:::\n\n\n## Use `nrow()` to see how many rows\n\n::: {.cell}\n\n```{.r .cell-code}\nfares_pounds |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 139943277\n```\n:::\n:::\n\n\n139 million rows of data\n\n## Use `head()` and `dplyr::collect()` to preview results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfares_pounds |>\n  head() |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 2\n  fare_amount fare_amount_pounds\n        <dbl>              <dbl>\n1        25.5              20.1 \n2        12                 9.48\n3        20                15.8 \n4         7                 5.53\n5         5                 3.95\n6        25.5              20.1 \n```\n:::\n:::\n\n\n## dplyr bindings\n\n![](images/dplyr_implemented.png)\n\n## What if a binding doesn't exist?\n\nFirst three trips in the dataset in 2021 where distance \\> 100 miles\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong_rides_2021 <- nyc_taxi |>\n  filter(year == 2021 & trip_distance > 100) |>\n  select(pickup_datetime, year, trip_distance)\n\nlong_rides_2021 |>\n  slice(1:3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"slice\"): no applicable method for 'slice' applied to an object of class \"arrow_dplyr_query\"\n```\n:::\n:::\n\n## head to the docs!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?arrow-dplyr\n```\n:::\n\n\n\n## A different function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong_rides_2021 |>\n  slice_head(n = 3) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 3\n  pickup_datetime      year trip_distance\n  <dttm>              <int>         <dbl>\n1 2021-01-03 09:01:26  2021          216.\n2 2021-01-03 11:36:52  2021          268.\n3 2021-01-06 07:27:55  2021          271.\n```\n:::\n:::\n\n\n## Or call `collect()` first\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong_rides_2021 |>\n  collect() |>\n  slice(1:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 3\n  pickup_datetime      year trip_distance\n  <dttm>              <int>         <dbl>\n1 2021-01-06 07:27:55  2021          271.\n2 2021-01-03 09:01:26  2021          216.\n3 2021-01-03 11:36:52  2021          268.\n```\n:::\n:::\n\n\n# Window functions\n\n## What are window functions?\n\n-   calculations within groups\n\n## Grouped summaries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfare_by_year <- nyc_taxi |>\n  filter(year %in% 2021:2022) |>\n  select(year, fare_amount)\n\nfare_by_year |>\n  group_by(year) |>\n  summarise(mean_fare = mean(fare_amount)) |> \n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 √ó 2\n   year mean_fare\n  <int>     <dbl>\n1  2021      13.5\n2  2022      13.0\n```\n:::\n:::\n\n\n## Window functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfare_by_year |>\n  group_by(year) |>\n  mutate(mean_fare = mean(fare_amount)) |> \n  collect()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: window functions not currently supported in Arrow\nCall collect() first to pull data into R.\n```\n:::\n:::\n\n\n## DuckDB\n\n![](images/dplyr-arrow-duckdb.png)\n\n## Window functions - via duckdb\n\n<!-- -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\n\nfare_by_year |>\n  group_by(year) |>\n  to_duckdb() |>\n  mutate(mean_fare = mean(fare_amount)) |> \n  to_arrow() |>\n  arrange(desc(fare_amount)) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 36,345,864 √ó 3\n    year fare_amount mean_fare\n   <int>       <dbl>     <dbl>\n 1  2021     818283.      13.5\n 2  2022     401092.      13.0\n 3  2021     398466.      13.5\n 4  2021     395854.      13.5\n 5  2021       6965       13.5\n 6  2021       6960.      13.5\n 7  2021       6010       13.5\n 8  2021       5954       13.5\n 9  2021       4969       13.5\n10  2021       3555.      13.5\n# ‚Ñπ 36,345,854 more rows\n```\n:::\n:::\n\n\n# Parquet\n\n## Parquet\n\n![](images/parquet.png)\n\n-   usually smaller than equivalent CSV file\n-   rich type system & stores the metadata along with the data\n-   \"column-oriented\" == better performance over CSV's row-by-row\n-   \"row-chunked\" == work on different parts of the file at the same time or skip some chunks all together\n\n::: notes\n-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory\n-   CSV has no info about data types, inferred by each parser\n:::\n\n## Let's compare!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc_taxi |>\n  write_dataset(\n    \"~/data/nyc-taxi-csv\",\n    format = \"csv\", \n    partitioning = c(\"year\", \"month\")\n  )\n```\n:::\n\n\n## Comparing size on disk\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_file_size <- function(path){\n  all_files_in_dir <- fs::dir_ls(path, recurse = TRUE)\n  sum(fs::file_size(all_files_in_dir))\n}\n```\n:::\n\n\n## How much space do the CSV and Parquet versions of the same files take up on disk?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parquet files\ntotal_file_size(\"~/data/nyc-taxi/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n37.5G\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# CSV files\ntotal_file_size(\"~/data/nyc-taxi-csv/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n158G\n```\n:::\n:::\n\n\n## Comparing analysis speed - Parquet\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parquet\ntic()\nopen_dataset(\"~/data/nyc-taxi\") |>\n  filter(year %in% 2014:2017) |>\n  group_by(year) |>\n  summarise(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n:::\n\n\n> 2.54 sec elapsed\n\n## Comparing analysis speed - CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#CSV\ntic()\nopen_dataset(\"~/data/nyc-taxi-csv\", format = \"csv\") |>\n  filter(year %in% 2014:2017) |>\n  group_by(year) |>\n  summarise(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n:::\n\n> 49.788 sec elapsed\n\n## Parquet file metadata\n\n![Source: https://parquet.apache.org/docs/file-format/metadata/](images/parquet_metadata.gif)\n\n# Partitioning\n\n- Data is saved in directories representing columns in the data\n\n![](images/partitions.png)\n\n\n## What variables should we use for partitioning?\n\nWhat happens if we just partition by year and not month?\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc_taxi |>\n  write_dataset(\"~/data/nyc-taxi-year\", partitioning = \"year\")\n```\n:::\n\n\nLet's compare!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyear_part <- open_dataset(\"~/data/nyc-taxi-year\")\nyear_month_part <- open_dataset(\"~/data/nyc-taxi\")\n```\n:::\n\n\n## Partitioned by year\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\n year_part |>\n filter(year %in% 2014:2017) |>\n  group_by(year, month) |>\n  summarise(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n:::\n\n> 8.338 sec elapsed\n\n## Partitioned by both month and year\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\n year_month_part |>\n filter(year %in% 2014:2017) |>\n  group_by(year, month) |>\n  summarise(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\ntoc()\n```\n:::\n\n> 5.621 sec elapsed\n\n## Art & Science of Partitioning\n\n-   Number of partitions also important (Arrow reads the metadata of each file)\n-   avoid files \\< 20MB and \\> 2GB\n-   avoid \\> 10,000 files (ü§Ø)\n-   partition on variables used in `filter()`\n\n# Summary\n\n1.  Arrow lets you work with larger-than-memory data directly from R using the dplyr API\n2.  Store files in Parquet format for better performance\n3.  Partition data based on your analysis workflows\n\n\n```{=html}\n<!-- Need to add in all of Danielle's acknowledgements\n\n\n\n[![Image from \"Larger-Than-Memory Data Workflows with Apache Arrow\" by Danielle Navarro is licensed under CC BY-ND 4.0](https://github.com/djnavarro/arrow-user2022/blob/main/img/arrow-libraries-structure.png?raw=true)](https://arrow-user2022.netlify.app/)\n\n## The arrow R package\n\n[![Image from \"Larger-Than-Memory Data Workflows with Apache Arrow\" by Danielle Navarro is licensed under CC BY-ND 4.0](https://github.com/djnavarro/arrow-user2022/blob/main/img/dplyr-backend.png?raw=true)](https://arrow-user2022.netlify.app/)\n\n-->\n```\n\n\n# Resources\n\n## Docs\n\n[![https://arrow.apache.org/docs/r/](images/docs.png)](https://arrow.apache.org/docs/r/)\n\n## Cookbook\n\n[![https://arrow.apache.org/cookbook/r/](images/cookbook.png)](https://arrow.apache.org/cookbook/r/)\n\n## Cheatsheet\n\n[![https://github.com/apache/arrow/blob/main/r/cheatsheet/arrow-cheatsheet.pdf](https://arrow.apache.org/img/20220427-arrow-r-cheatsheet-thumbnail.png)](https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf)\n\n\n```{=html}\n<!-- \nThe Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package‚Äôs main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full Arrow for R package documentation and articles and the Arrow Cookbook, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the Apache Arrow visual identity guidance.\n-->\n```\n\n## UseR! 2022 Tutorial\n\n[![https://arrow-user2022.netlify.app/](images/usertutorial.png)](https://arrow-user2022.netlify.app/)\n\n<!-- Also mention mini taxi dataset -->\n\n## Awesome Arrow\n\n[![https://github.com/thisisnic/awesome-arrow-r](images/awesomearrow.png)](https://github.com/thisisnic/awesome-arrow-r)\n\n# Get Involved!\n\n## Open an issue\n\n[![https://github.com/apache/arrow/issues/](images/issues.png)](https://github.com/apache/arrow/issues/)\n\n## Make a PR!\n\n-   docs\n-   cookbook\n-   code\n\n# Efficiently Engineering Bigger Data with Arrow",
    "supporting": [
      "slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}